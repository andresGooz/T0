{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Regression by hand\n",
    ">a) Escriba una función que calcule la función de pérdida, error cuadrático medio (MSE - mean squared error), para un dato o para un conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as n\n",
    "def MSE(a,can,pos):\n",
    "    result=0.0\n",
    "    n=len(a)\n",
    "    N=float(len(a))\n",
    "    for i in range(pos,pos+can):\n",
    "        result=result+1/N*((a[i]-sum(a)/N)**2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">b) Escriba una función que calcule el gradiente (derivada) de la función de pérdida anterior, para un dato o para un conjunto de datos. Escriba explícitamente la derivada (gradiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def Gra(x,m,n,y,j):\n",
    "    a=2.0\n",
    "    b=13.0\n",
    "    c=a/b\n",
    "    a=0.0\n",
    "    b=0.0\n",
    "    for i in range (0,len(x)):\n",
    "        a+=c*x[i]*m[i]\n",
    "        b+=c*x[i]*m[i]\n",
    "    a=(a + c * (n-y))*x[j]\n",
    "    b=b+ c * (n-y)\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">c) Escriba una función que calcule los parámetros de una regresión lineal simple de manera analítica (es decir el mínimo global)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "from numpy.linalg import inv\n",
    "def regresionLineal(MATRIZX,MATRIZY,cantidad):\n",
    "    g=0\n",
    "    n=0\n",
    "    h=[0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    MATRIZX=MATRIZX[:cantidad]\n",
    "    MATRIZY=MATRIZY[:cantidad]\n",
    "    m=numpy.dot(numpy.dot( inv(numpy.dot(MATRIZX.transpose(),MATRIZX)), MATRIZX.transpose()),MATRIZY)\n",
    "    for i in range(0,cantidad):\n",
    "        g=g+MATRIZY[i]\n",
    "    for i in range(0,cantidad):\n",
    "        for j in range(0,13):\n",
    "            h[j]=h[j]-MATRIZX[i][j]\n",
    "    for i in range(0,13):\n",
    "        n=h[i]/cantidad*m[i]+n\n",
    "    n=g/cantidad-n\n",
    "    return (n,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">d) Ahora escriba un programa que permita entrenar una regresión lineal a través del algoritmo SGD mostrado en la ecuación del algoritmo SGD, es decir, que de manera iterativa, vaya tomando un dato a la vez, y actualizando el parámetro  𝛽  a través del gradiente descendiente de la función de pérdida de la regresión lineal ordinaria, de la pregunta b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def SGD(x,y,can, ene, num_iter):\n",
    "    m=[1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "    n=1.0\n",
    "    for l in range(0,can):\n",
    "        for j in range(0,13):\n",
    "            for i in range(0,num_iter):\n",
    "                error1,error2=Gra(x[l],m,n,y[l],j)\n",
    "                m[j] = (m[j] - error1*ene)\n",
    "                n = n - error2*ene\n",
    "                \n",
    "    return(n,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">e) Demuestre que sus programas funcionan en un problema de regresión simple. Para esto utilice el dataset Boston Housing , disponible a través de la librería sklearn, el cual corresponde a el precio de diferentes casas en Boston además de distintas características relevantes respecto al lugar, como por ejemplo el crimen en la ciudad, el número de habitaciones, que tan vieja es, distancia a lugares relevantes, entre otros. Éstas características deben combinarse linealmente para estimar el precio de la casa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "X_train,y_train = load_boston(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">e) Varié la tasa de aprendizaje  𝜂∈[0,1]  del algoritmo SGD del punto d), compare los resultados entre sí y con la solución óptima encontrada en c). Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "(n1,m1)=SGD(X_train,y_train,300,0.2,1000)\n",
    "t1=(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "(n2,m2)=SGD(X_train,y_train,300,0.6,1000)\n",
    "t2=(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "(n3,m3)=SGD(X_train,y_train,300,0.8,1000)\n",
    "t3=(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "(n4,m4)=SGD(X_train,y_train,300,1,1000)\n",
    "t4=(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "(n5,m5)=regresionLineal(X_train,y_train,300)\n",
    "t5=(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba(n,m):\n",
    "    h=0\n",
    "    k=[0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for i in range(300,500):\n",
    "        h=h+y_train[i]\n",
    "        for j in range(0,13):\n",
    "            k[j]=k[j]+X_train[i][j]\n",
    "    for i in range(0,13):\n",
    "        k[i]=k[i]/200\n",
    "    h=h/200\n",
    "    p=0\n",
    "    for i in range(0,13):\n",
    "        p=m[i]*k[i]\n",
    "    p=p+n\n",
    "    print str(p)+\"=\"+str(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.77347466360595=18.192999999999994\n",
      "La prueba 1 se demoro0:00:46.738876\n",
      "28.773474663607644=18.192999999999994\n",
      "La prueba 2 se demoro0:00:46.305810\n",
      "28.773474663608006=18.192999999999994\n",
      "La prueba 3 se demoro0:00:46.462767\n",
      "-6.226301892483497e+197=18.192999999999994\n",
      "La prueba 4 se demoro0:00:48.541763\n",
      "50.30148464858562=18.192999999999994\n",
      "La prueba 5 se demoro0:00:00.005356\n"
     ]
    }
   ],
   "source": [
    "prueba(n1,m1)\n",
    "print \"La prueba 1 se demoro\"+str(t1)\n",
    "prueba(n2,m2)\n",
    "print \"La prueba 2 se demoro\"+str(t2)\n",
    "prueba(n3,m3)\n",
    "print \"La prueba 3 se demoro\"+str(t3)\n",
    "prueba(n4,m4)\n",
    "print \"La prueba 4 se demoro\"+str(t4)\n",
    "prueba(n5,m5)\n",
    "print \"La prueba 5 se demoro\"+str(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión lineal llega a un valor cercano al real y sin tener que emplear mucho tiempo en crearlo ni en ejecutarlo, por otro lado esta el SGD el cual consume considerablemente mas recursos y tiempo. En el learning rate se puede ver que la diferencia que hace es poca, siendo la con menor learning rate mas exacta, pero con learning rate de 1 la función diverge, por lo cual no nos es de utilidad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
