{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Regression by hand\n",
    ">a) Escriba una funci贸n que calcule la funci贸n de p茅rdida, error cuadr谩tico medio (MSE - mean squared error), para un dato o para un conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as n\n",
    "def MSE(a,can,pos):\n",
    "    result=0.0\n",
    "    n=len(a)\n",
    "    N=float(len(a))\n",
    "    for i in range(pos,pos+can):\n",
    "        result=result+1/N*((a[i]-sum(a)/N)**2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">b) Escriba una funci贸n que calcule el gradiente (derivada) de la funci贸n de p茅rdida anterior, para un dato o para un conjunto de datos. Escriba expl铆citamente la derivada (gradiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def Gra(x,m,n,y,j):\n",
    "    a=2.0\n",
    "    b=13.0\n",
    "    c=a/b\n",
    "    a=0.0\n",
    "    b=0.0\n",
    "    for i in range (0,len(x)):\n",
    "        a+=c*x[i]*m[i]\n",
    "        b+=c*x[i]*m[i]\n",
    "    a=(a + c * (n-y))*x[j]\n",
    "    b=b+ c * (n-y)\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">c) Escriba una funci贸n que calcule los par谩metros de una regresi贸n lineal simple de manera anal铆tica (es decir el m铆nimo global)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "from numpy.linalg import inv\n",
    "def regresionLineal(MATRIZX,MATRIZY,cantidad):\n",
    "    g=0\n",
    "    n=0\n",
    "    h=[0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    MATRIZX=MATRIZX[:cantidad]\n",
    "    MATRIZY=MATRIZY[:cantidad]\n",
    "    m=numpy.dot(numpy.dot( inv(numpy.dot(MATRIZX.transpose(),MATRIZX)), MATRIZX.transpose()),MATRIZY)\n",
    "    for i in range(0,cantidad):\n",
    "        g=g+MATRIZY[i]\n",
    "    for i in range(0,cantidad):\n",
    "        for j in range(0,13):\n",
    "            h[j]=h[j]-MATRIZX[i][j]\n",
    "    for i in range(0,13):\n",
    "        n=h[i]/cantidad*m[i]+n\n",
    "    n=g/cantidad-n\n",
    "    return (n,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">d) Ahora escriba un programa que permita entrenar una regresi贸n lineal a trav茅s del algoritmo SGD mostrado en la ecuaci贸n del algoritmo SGD, es decir, que de manera iterativa, vaya tomando un dato a la vez, y actualizando el par谩metro    a trav茅s del gradiente descendiente de la funci贸n de p茅rdida de la regresi贸n lineal ordinaria, de la pregunta b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def SGD(x,y,can, ene, num_iter):\n",
    "    m=[1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "    n=1.0\n",
    "    for l in range(0,can):\n",
    "        for j in range(0,13):\n",
    "            for i in range(0,num_iter):\n",
    "                error1,error2=Gra(x[l],m,n,y[l],j)\n",
    "                m[j] = (m[j] - error1*ene)\n",
    "                n = n - error2*ene\n",
    "                \n",
    "    return(n,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">e) Demuestre que sus programas funcionan en un problema de regresi贸n simple. Para esto utilice el dataset Boston Housing , disponible a trav茅s de la librer铆a sklearn, el cual corresponde a el precio de diferentes casas en Boston adem谩s de distintas caracter铆sticas relevantes respecto al lugar, como por ejemplo el crimen en la ciudad, el n煤mero de habitaciones, que tan vieja es, distancia a lugares relevantes, entre otros. stas caracter铆sticas deben combinarse linealmente para estimar el precio de la casa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "X_train,y_train = load_boston(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">e) Vari茅 la tasa de aprendizaje  [0,1]  del algoritmo SGD del punto d), compare los resultados entre s铆 y con la soluci贸n 贸ptima encontrada en c). Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "(n1,m1)=SGD(X_train,y_train,300,0.2,1000)\n",
    "t1=(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "(n2,m2)=SGD(X_train,y_train,300,0.6,1000)\n",
    "t2=(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "(n3,m3)=SGD(X_train,y_train,300,0.8,1000)\n",
    "t3=(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "(n4,m4)=SGD(X_train,y_train,300,1,1000)\n",
    "t4=(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "(n5,m5)=regresionLineal(X_train,y_train,300)\n",
    "t5=(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba(n,m):\n",
    "    h=0\n",
    "    k=[0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for i in range(300,500):\n",
    "        h=h+y_train[i]\n",
    "        for j in range(0,13):\n",
    "            k[j]=k[j]+X_train[i][j]\n",
    "    for i in range(0,13):\n",
    "        k[i]=k[i]/200\n",
    "    h=h/200\n",
    "    p=0\n",
    "    for i in range(0,13):\n",
    "        p=m[i]*k[i]\n",
    "    p=p+n\n",
    "    print str(p)+\"=\"+str(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.77347466360595=18.192999999999994\n",
      "La prueba 1 se demoro0:00:46.738876\n",
      "28.773474663607644=18.192999999999994\n",
      "La prueba 2 se demoro0:00:46.305810\n",
      "28.773474663608006=18.192999999999994\n",
      "La prueba 3 se demoro0:00:46.462767\n",
      "-6.226301892483497e+197=18.192999999999994\n",
      "La prueba 4 se demoro0:00:48.541763\n",
      "50.30148464858562=18.192999999999994\n",
      "La prueba 5 se demoro0:00:00.005356\n"
     ]
    }
   ],
   "source": [
    "prueba(n1,m1)\n",
    "print \"La prueba 1 se demoro\"+str(t1)\n",
    "prueba(n2,m2)\n",
    "print \"La prueba 2 se demoro\"+str(t2)\n",
    "prueba(n3,m3)\n",
    "print \"La prueba 3 se demoro\"+str(t3)\n",
    "prueba(n4,m4)\n",
    "print \"La prueba 4 se demoro\"+str(t4)\n",
    "prueba(n5,m5)\n",
    "print \"La prueba 5 se demoro\"+str(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresi贸n lineal llega a un valor cercano al real y sin tener que emplear mucho tiempo en crearlo ni en ejecutarlo, por otro lado esta el SGD el cual consume considerablemente mas recursos y tiempo. En el learning rate se puede ver que la diferencia que hace es poca, siendo la con menor learning rate mas exacta, pero con learning rate de 1 la funci贸n diverge, por lo cual no nos es de utilidad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
